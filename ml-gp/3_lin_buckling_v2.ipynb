{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Buckilng of Panel V2\n",
    "use window kernel?\n",
    "do cylinder buckling next?\n",
    "can try sum of sinusoidal / periodic modes? That would probably be great for this problem (almost cheating if it works).\n",
    "\n",
    "I believe the reason why it didn't work as well in the first approach to solve linear buckling is that the kernel functions may not\n",
    "work well for eigenmode solving. Same thing happened with the PINN eigenvalue solver where it didn't work well and gave one half wave soln that doesn't satisfy BCs when I weakly enforced BCs. Needs shorter length scale but then regularization is failing with that. Instead, I can strongly enforce BCs with my kernel too (instead of weakly enforcing it like I am now).\n",
    "\n",
    "Try using the sin^2 periodic kernel \n",
    "$$ k(x,x') = \\sigma^2 \\textrm{exp}(- \\frac{2}{L^2} sin^2(\\frac{\\pi}{p} |x-x'|)) $$\n",
    "Or try the bounded kernel idea I had:\n",
    "$$ k(x,x') = \\textrm{exp}(-\\frac{(x-x')^2}{2L^2}) \\cdot g(x) g(x') $$\n",
    "where $g(x) = x(a-x)$. And do same for $y$ direction with $h(y) = y (b-y)$ as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_load_factor=31.633294000320518\n"
     ]
    }
   ],
   "source": [
    "# panel linear static inputs\n",
    "# buckling inputs\n",
    "# ----------------\n",
    "\n",
    "# aluminum plate material properties\n",
    "E = 70e9; nu = 0.3; thick = 0.005\n",
    "D = E * thick**3 / 12.0 / (1 - nu**2)\n",
    "(a, b) = (3, 1)\n",
    "\n",
    "axial = True\n",
    "if axial:\n",
    "    Nxx, Nxy, Nyy = (1e3, 0, 0)\n",
    "else:\n",
    "    Nxx, Nxy, Nyy = (0, 1e2, 0)\n",
    "\n",
    "# choose kernel hyperparameters\n",
    "Lx = 0.4; Ly = 0.4\n",
    "# took out smaller lower length scale bc cause regularization to fail for now\n",
    "\n",
    "# predict the buckling load factor (BLF)\n",
    "pi = 3.14159\n",
    "if axial:\n",
    "    pred_buckling_load = pi**2 * D / b**2 * 4.0 \n",
    "    pred_load_factor = pred_buckling_load / Nxx\n",
    "    sigma = 0.5 * pred_load_factor\n",
    "    print(f\"{pred_load_factor=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just comment this symbolic code out since don't want to run this each time I'm doing analysis\n",
    "\n",
    "# first compute all the kernel derivatives\n",
    "import sympy as sym\n",
    "\n",
    "x = sym.Symbol('x')\n",
    "xp = sym.Symbol('xp')\n",
    "L = sym.Symbol('L')\n",
    "a = sym.Symbol('a')\n",
    "\n",
    "run_symbolic = False\n",
    "\n",
    "if run_symbolic:\n",
    "\n",
    "    kernel = sym.exp(-0.5 / L / L * (x - xp)**2) * x * (a-x) * xp * (a - xp)\n",
    "    print(f\"{kernel=}\")\n",
    "\n",
    "    lapl2_kernel = kernel.diff(x, x)\n",
    "    lapl2_kernel = sym.simplify(lapl2_kernel)\n",
    "    print(f\"{lapl2_kernel=}\")\n",
    "\n",
    "    lapl4_kernel = kernel.diff(x, x, x, x)\n",
    "    lapl4_kernel = sym.simplify(lapl4_kernel)\n",
    "    print(f\"{lapl4_kernel=}\")\n",
    "\n",
    "    lapl6_kernel = kernel.diff(x, x, x, x, xp, xp)\n",
    "    lapl6_kernel = sym.simplify(lapl6_kernel)\n",
    "    print(f\"{lapl6_kernel=}\")\n",
    "\n",
    "    lapl8_kernel = kernel.diff(x, x, x, x, xp, xp, xp, xp)\n",
    "    lapl8_kernel = sym.simplify(lapl8_kernel)\n",
    "    print(f\"{lapl8_kernel=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 07:34:37.737010: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-04 07:34:37.766275: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-04 07:34:38.018370: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-04 07:34:38.259754: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1733315678.450747    9779 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1733315678.514396    9779 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-04 07:34:38.939346: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-04 07:34:48.858878: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Attempt to convert a value (a) with an unsupported type (<class 'sympy.core.symbol.Symbol'>) to a Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m ymin \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m; ymax \u001b[38;5;241m=\u001b[39m b\n\u001b[1;32m     31\u001b[0m lb \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant([xmin, ymin], dtype\u001b[38;5;241m=\u001b[39mDTYPE)\n\u001b[0;32m---> 32\u001b[0m ub \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mxmax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mymax\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# generate the collocation points on the interior\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgen_collocation_points\u001b[39m(num_domain, minval, maxval):\n",
      "File \u001b[0;32m~/miniconda3/envs/F2F/lib/python3.9/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142\u001b[0m, in \u001b[0;36mweak_tensor_binary_op_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    141\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mis_auto_dtype_conversion_enabled():\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    144\u001b[0m   bound_arguments\u001b[38;5;241m.\u001b[39mapply_defaults()\n",
      "File \u001b[0;32m~/miniconda3/envs/F2F/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:276\u001b[0m, in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstant\u001b[39m(\n\u001b[1;32m    179\u001b[0m     value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    180\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[ops\u001b[38;5;241m.\u001b[39mOperation, ops\u001b[38;5;241m.\u001b[39m_EagerTensorBase]:\n\u001b[1;32m    181\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;124;03m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 276\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mallow_broadcast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/F2F/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:289\u001b[0m, in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.constant\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    288\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m--> 289\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_eager_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m const_tensor \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39m_create_graph_constant(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    292\u001b[0m     value, dtype, shape, name, verify_shape, allow_broadcast\n\u001b[1;32m    293\u001b[0m )\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m const_tensor\n",
      "File \u001b[0;32m~/miniconda3/envs/F2F/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:301\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constant_eager_impl\u001b[39m(\n\u001b[1;32m    298\u001b[0m     ctx, value, dtype, shape, verify_shape\n\u001b[1;32m    299\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ops\u001b[38;5;241m.\u001b[39m_EagerTensorBase:\n\u001b[1;32m    300\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 301\u001b[0m   t \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_eager_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
      "File \u001b[0;32m~/miniconda3/envs/F2F/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:108\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    106\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    107\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Attempt to convert a value (a) with an unsupported type (<class 'sympy.core.symbol.Symbol'>) to a Tensor."
     ]
    }
   ],
   "source": [
    "# compute the linear static analysis inputs and mesh domain of collocation pts\n",
    "# ----------------------------------------------------------------------------\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# case = 'low'\n",
    "# case = 'medium'\n",
    "case = 'high'\n",
    "\n",
    "if case == 'high':\n",
    "    num_domain = 300\n",
    "    num_bndry = 100\n",
    "    num_test = 50\n",
    "elif case == 'medium':\n",
    "    num_domain = 40\n",
    "    num_bndry = 10\n",
    "    num_test = 50\n",
    "elif case == 'low':\n",
    "    num_domain = 10\n",
    "    num_bndry = 5\n",
    "    num_test = 5\n",
    "\n",
    "num_interior = num_domain\n",
    "DTYPE = tf.float32\n",
    "\n",
    "# set the boundary of the panel\n",
    "xmin = 0; xmax = a\n",
    "ymin = 0; ymax = b\n",
    "\n",
    "lb = tf.constant([xmin, ymin], dtype=DTYPE)\n",
    "ub = tf.constant([xmax, ymax], dtype=DTYPE)\n",
    "\n",
    "# generate the collocation points on the interior\n",
    "def gen_collocation_points(num_domain, minval, maxval):\n",
    "    x_r = tf.random.uniform((num_domain,1), minval[0], maxval[0], dtype=DTYPE)\n",
    "    y_r = tf.random.uniform((num_domain,1), minval[1], maxval[1], dtype=DTYPE)\n",
    "    X_r = tf.concat([x_r, y_r], axis=1)\n",
    "    # data_init = tf.random_uniform_initializer(minval=minval, maxval=maxval, seed=0)\n",
    "    # return tf.Variable(data_init(shape=[num_domain, 1]), dtype=tf.float32)\n",
    "    return tf.Variable(X_r, dtype=DTYPE)\n",
    "\n",
    "x_train = gen_collocation_points(num_domain, lb, ub)\n",
    "x_test = gen_collocation_points(num_test, lb, ub)\n",
    "print(f\"{type(x_train)=}\")\n",
    "\n",
    "x = x_train[:,0:1]\n",
    "y = x_train[:,1:2]\n",
    "\n",
    "x2 = x_test[:,0:1]\n",
    "y2 = x_test[:,1:2]\n",
    "\n",
    "# generate boundary domain points\n",
    "assert num_bndry % 2 == 0\n",
    "N_b = int(num_bndry / 2)\n",
    "x_b1 = tf.random.uniform((N_b,1), lb[0], ub[0], dtype=DTYPE)\n",
    "y_b1 = lb[1] + (ub[1] - lb[1]) * \\\n",
    "    tf.keras.backend.random_bernoulli((N_b,1), 0.5, dtype=DTYPE)\n",
    "X_b1 = tf.concat([x_b1, y_b1], axis=1)\n",
    "\n",
    "# boundary data on x edges\n",
    "y_b2 = tf.random.uniform((N_b,1), lb[1], ub[1], dtype=DTYPE)\n",
    "x_b2 = lb[0] + (ub[0] - lb[0]) * \\\n",
    "    tf.keras.backend.random_bernoulli((N_b,1), 0.5, dtype=DTYPE)\n",
    "# print(f\"{x_b2=}\")\n",
    "X_b2 = tf.concat([x_b2, y_b2], axis=1)\n",
    "# print(f\"{X_b2=}\")\n",
    "# print(f\"{x_b2=}\")\n",
    "\n",
    "x_bndry = tf.Variable(tf.concat([X_b1, X_b2], axis=0), dtype=DTYPE)\n",
    "\n",
    "# plot the data to check\n",
    "plt.scatter(x_bndry[:,0], x_bndry[:,1])\n",
    "plt.scatter(x_train[:,0], x_train[:,1]) # only show 1000 of the points\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define all the derivatives here\n",
    "# now it's non-stationary so uses x,xp not xbar\n",
    "def d2_fact(x, xp, a, L):\n",
    "    return -xp*(a - xp)*(2*L**4 + 2.0*L**2*(a - 2*x)*(x - xp) + 1.0*x*(L**2 - (x - xp)**2)*(a - x)) / L**4\n",
    "\n",
    "def d4_fact(x, xp, a, L):\n",
    "    return xp*(a - xp)*(12.0*L**6 - 12.0*L**4*(x - xp)**2 + 4*L**2*(3.0*L**2 - 1.0*(x - xp)**2)*(a - 2*x)* \\\n",
    "                        (x - xp) + x*(a - x)*(3.0*L**4 - 6.0*L**2*(x - xp)**2 + 1.0*(x - xp)**4))/L**8\n",
    "\n",
    "def d6_fact(x, xp, a, L):\n",
    "    return (-24.0*L**10 + L**8*(-24.0*xp*(a - xp) - 48.0*xp*(x - xp) + 48.0*(a - xp)*(x - xp) +\\\n",
    "            24.0*(x - xp)**2) + L**6*(-8*x*xp*(3.0*L**2 - 1.0*(x - xp)**2) + 24.0*x*xp*(a - xp)*(x - xp) +\\\n",
    "            16.0*x*xp*(x - xp)**2 + 8*x*(3.0*L**2 - 1.0*(x - xp)**2)*(a - xp) + 8*x*(3.0*L**2 - 1.0*(x - xp)**2)*(x - xp) -\\\n",
    "            16.0*x*(a - xp)*(x - xp)**2 - 12.0*xp*(L**2 - (x - xp)**2)*(a - xp) - 24.0*xp*(L**2 - (x - xp)**2)* \\\n",
    "            (x - xp) + 8*xp*(3.0*L**2 - 1.0*(x - xp)**2)*(a - x) - 24.0*xp*(a - x)*(a - xp)*(x - xp) - 16.0*xp*(a - x)* \\\n",
    "            (x - xp)**2 + 48.0*xp*(a - xp)*(x - xp)**2 + 24.0*(L**2 - (x - xp)**2)*(a - xp)*(x - xp) - 8*(3.0*L**2 -\\\n",
    "            1.0*(x - xp)**2)*(a - x)*(a - xp) - 8*(3.0*L**2 - 1.0*(x - xp)**2)*(a - x)*(x - xp) +\\\n",
    "            16.0*(a - x)*(a - xp)*(x - xp)**2) + L**4*(-12.0*x*xp*(L**2 - (x - xp)**2)*(a - x)*(a - xp) +\\\n",
    "            12.0*x*xp*(3.0*L**2 - 1.0*(x - xp)**2)*(a - xp)*(x - xp) + 8.0*x*xp*(3.0*L**2 - 1.0*(x - xp)**2)*(x - xp)**2 -\\\n",
    "            2*x*xp*(12.0*L**2 - 4.0*(x - xp)**2)*(a - x)*(x - xp) - 16.0*x*xp*(a - xp)*(x - xp)**3 - 8.0*x*(3.0*L**2 -\\\n",
    "            1.0*(x - xp)**2)*(a - xp)*(x - xp)**2 + 2*x*(12.0*L**2 - 4.0*(x - xp)**2)*(a - x)*(a - xp)*(x - xp) +\\\n",
    "            12.0*xp*(L**2 - (x - xp)**2)*(a - xp)*(x - xp)**2 - 12.0*xp*(3.0*L**2 - 1.0*(x - xp)**2)*(a - x)*(a - xp)*(x - xp) -\\\n",
    "            8.0*xp*(3.0*L**2 - 1.0*(x - xp)**2)*(a - x)*(x - xp)**2 + 16.0*xp*(a - x)*(a - xp)*(x - xp)**3 + 8.0*(3.0*L**2 -\\\n",
    "            1.0*(x - xp)**2)*(a - x)*(a - xp)*(x - xp)**2) + L**2*x*(a - x)*(3.0*L**4 - 6.0*L**2*(x - xp)**2 + 1.0*(x - xp)**4)* \\\n",
    "            (-1.0*xp*(a - xp) - 2.0*xp*(x - xp) + 2.0*(a - xp)*(x - xp)) + L**2*(-2*L**2*x*(a - x)*(3.0*L**4 - 6.0*L**2*(x - xp)**2 + \\\n",
    "            1.0*(x - xp)**4) - 4.0*x*xp*(3.0*L**2 - 1.0*(x - xp)**2)*(a - xp)*(x - xp)**3 + 2.0*x*xp*(12.0*L**2 -\\\n",
    "            4.0*(x - xp)**2)*(a - x)*(a - xp)*(x - xp)**2 + 4.0*xp*(3.0*L**2 - 1.0*(x - xp)**2)*(a - x)*(a - xp)*(x - xp)**3) +\\\n",
    "            1.0*x*xp*(a - x)*(a - xp)*(x - xp)**2*(3.0*L**4 - 6.0*L**2*(x - xp)**2 + 1.0*(x - xp)**4))/L**12\n",
    "\n",
    "def d8_fact(x, xp, a, L):\n",
    "    return (432.0*L**12 + 240.0*L**10*a**2 - 300.0*L**10*a*x - 300.0*L**10*a*xp - 2484.0*L**10*x**2 + 5568.0*L**10*x*xp -\\\n",
    "        2484.0*L**10*xp**2 - 1140.0*L**8*a**2*x**2 + 2385.0*L**8*a**2*x*xp - 1140.0*L**8*a**2*xp**2 + 1320.0*L**8*a*x**3 -\\\n",
    "        1425.0*L**8*a*x**2*xp - 1425.0*L**8*a*x*xp**2 + 1320.0*L**8*a*xp**3 + 1644.0*L**8*x**4 - 9216.0*L**8*x**3*xp +\\\n",
    "        15249.0*L**8*x**2*xp**2 - 9216.0*L**8*x*xp**3 + 1644.0*L**8*xp**4 + 660.0*L**6*a**2*x**4 - 3060.0*L**6*a**2*x**3*xp +\\\n",
    "        4800.0*L**6*a**2*x**2*xp**2 - 3060.0*L**6*a**2*x*xp**3 + 660.0*L**6*a**2*xp**4 - 720.0*L**6*a*x**5 + 2580.0*L**6*a*x**4*xp -\\\n",
    "        1860.0*L**6*a*x**3*xp**2 - 1860.0*L**6*a*x**2*xp**3 + 2580.0*L**6*a*x*xp**4 - 720.0*L**6*a*xp**5 - 276.0*L**6*x**6 +\\\n",
    "        3096.0*L**6*x**5*xp - 10320.0*L**6*x**4*xp**2 + 15000.0*L**6*x**3*xp**3 - 10320.0*L**6*x**2*xp**4 + 3096.0*L**6*x*xp**5 -\\\n",
    "        276.0*L**6*xp**6 - 100.0*L**4*a**2*x**6 + 810.0*L**4*a**2*x**5*xp - 2340.0*L**4*a**2*x**4*xp**2 + 3260.0*L**4*a**2*x**3*xp**3 -\\\n",
    "        2340.0*L**4*a**2*x**2*xp**4 + 810.0*L**4*a**2*x*xp**5 - 100.0*L**4*a**2*xp**6 + 104.0*L**4*a*x**7 - 730.0*L**4*a*x**6*xp +\\\n",
    "        1566.0*L**4*a*x**5*xp**2 - 940.0*L**4*a*x**4*xp**3 - 940.0*L**4*a*x**3*xp**4 + 1566.0*L**4*a*x**2*xp**5 - 730.0*L**4*a*x*xp**6 +\\\n",
    "        104.0*L**4*a*xp**7 + 12.0*L**4*x**8 - 304.0*L**4*x**7*xp + 1794.0*L**4*x**6*xp**2 - 4632.0*L**4*x**5*xp**3 + \\\n",
    "        6260.0*L**4*x**4*xp**4 - 4632.0*L**4*x**3*xp**5 + 1794.0*L**4*x**2*xp**6 - 304.0*L**4*x*xp**7 + 12.0*L**4*xp**8 +\\\n",
    "        4.0*L**2*a**2*x**8 - 60.0*L**2*a**2*x**7*xp + 280.0*L**2*a**2*x**6*xp**2 - 644.0*L**2*a**2*x**5*xp**3 + 840.0*L**2*a**2*x**4*xp**4 - \\\n",
    "        644.0*L**2*a**2*x**3*xp**5 + 280.0*L**2*a**2*x**2*xp**6 - 60.0*L**2*a**2*x*xp**7 + 4.0*L**2*a**2*xp**8  4.0*L**2*a*x**9 + \\\n",
    "        56.0*L**2*a*x**8*xp - 220.0*L**2*a*x**7*xp**2 + 364.0*L**2*a*x**6*xp**3 - 196.0*L**2*a*x**5*xp**4 - 196.0*L**2*a*x**4*xp**5 + \\\n",
    "        364.0*L**2*a*x**3*xp**6 - 220.0*L**2*a*x**2*xp**7 + 56.0*L**2*a*x*xp**8 - 4.0*L**2*a*xp**9 + 8.0*L**2*x**9*xp - \\\n",
    "        92.0*L**2*x**8*xp**2 + 392.0*L**2*x**7*xp**3 - 868.0*L**2*x**6*xp**4 + 1120.0*L**2*x**5*xp**5 - 868.0*L**2*x**4*xp**6 + \\\n",
    "        392.0*L**2*x**3*xp**7 - 92.0*L**2*x**2*xp**8 + 8.0*L**2*x*xp**9 + 1.0*a**2*x**9*xp - 8.0*a**2*x**8*xp**2 + \\\n",
    "        28.0*a**2*x**7*xp**3 - 56.0*a**2*x**6*xp**4 + 70.0*a**2*x**5*xp**5 - 56.0*a**2*x**4*xp**6 + 28.0*a**2*x**3*xp**7 - \\\n",
    "        8.0*a**2*x**2*xp**8 + 1.0*a**2*x*xp**9 - 1.0*a*x**10*xp + 7.0*a*x**9*xp**2 - 20.0*a*x**8*xp**3 + 28.0*a*x**7*xp**4 - \\\n",
    "        14.0*a*x**6*xp**5 - 14.0*a*x**5*xp**6 + 28.0*a*x**4*xp**7 - 20.0*a*x**3*xp**8 + 7.0*a*x**2*xp**9 - \\\n",
    "        1.0*a*x*xp**10 + 1.0*x**10*xp**2 - 8.0*x**9*xp**3 + 28.0*x**8*xp**4 - 56.0*x**7*xp**5 + 70.0*x**6*xp**6 - 56.0*x**5*xp**7 + \\\n",
    "        28.0*x**4*xp**8 - 8.0*x**3*xp**9 + 1.0*x**2*xp**10)/L**16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2416012696.py, line 79)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[13], line 79\u001b[0;36m\u001b[0m\n\u001b[0;31m    )\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# now define the main kernels and derivative kernels for 2d case\n",
    "\n",
    "L_tf = tf.constant(np.array([Lx, Ly]), dtype=DTYPE)\n",
    "def g(x,a): # 1d level set\n",
    "    return x * (a-x)\n",
    "\n",
    "def kernel2d_tf(x, xp):\n",
    "    # x input is N x 1 x 2 array, xp is 1 x M x 2 array\n",
    "    # xbar is then an N x M x 2 shape array\n",
    "    # print(f\"{x=} {L_tf=}\")\n",
    "    xbar = (x - xp) / L_tf\n",
    "    x1 = x[:,:,0:1]\n",
    "    x2 = x[:,:,1:2]\n",
    "    x1p = xp[:,:,0:1]\n",
    "    x2p = xp[:,:,1:2]\n",
    "    # output is N x M matrix of kernel matrix\n",
    "    return tf.exp(-0.5 * tf.reduce_sum(tf.pow(xbar, 2.0), axis=-1)) * g(x1,a) * g(x1p,a) * g(x2,b) * g(x2p,b)\n",
    "\n",
    "def dx2_kernel_tf(x, xp):\n",
    "    # first two dimensions are NxN matrix parts\n",
    "    # x is N x 1 x 2 matrix, xp is 1 x M x 2 matrix\n",
    "    Lx = L_tf[0]; Ly = L_tf[1]\n",
    "    x1 = x[:,:,0:1]\n",
    "    x2 = x[:,:,1:2]\n",
    "    x1p = xp[:,:,0:1]\n",
    "    x2p = xp[:,:,1:2]\n",
    "\n",
    "    # baseline kernel matrix which we will scale and add some terms\n",
    "    K = kernel2d_tf(x,xp)\n",
    "\n",
    "    # TODO : shear or general case later (just axial now)\n",
    "    return K * (d2_fact(x1, x1p, a, Lx))\n",
    "\n",
    "def doubledx2_kernel_tf(x, xp):\n",
    "    # first two dimensions are NxN matrix parts\n",
    "    # x is N x 1 x 2 matrix, xp is 1 x M x 2 matrix\n",
    "    Lx = L_tf[0]; Ly = L_tf[1]\n",
    "\n",
    "    x1 = x[:,:,0:1]\n",
    "    x2 = x[:,:,1:2]\n",
    "    x1p = xp[:,:,0:1]\n",
    "    x2p = xp[:,:,1:2]\n",
    "\n",
    "    # baseline kernel matrix which we will scale and add some terms\n",
    "    K = kernel2d_tf(x,xp)\n",
    "\n",
    "    # TODO : shear or general case later (just axial now)\n",
    "    return K * (d4_fact(x1, x1p, a, Lx))\n",
    "\n",
    "def kernel2d_bilapl_tf(x, xp):\n",
    "    # first two dimensions are NxN matrix parts\n",
    "    # x is N x 1 x 2 matrix, xp is 1 x M x 2 matrix\n",
    "    Lx = L_tf[0]; Ly = L_tf[1]\n",
    "    x1 = x[:,:,0:1]\n",
    "    x2 = x[:,:,1:2]\n",
    "    x1p = xp[:,:,0:1]\n",
    "    x2p = xp[:,:,1:2]\n",
    "\n",
    "    # baseline kernel matrix which we will scale and add some terms\n",
    "    K = kernel2d_tf(x,xp)\n",
    "\n",
    "    return K * (d4_fact(x1, x1p, a,Lx) + 2.0 * d2_fact(x1, x1p, a,Lx) * d2_fact(x2, x2p, b, Ly) + d4_fact(x2, x2p, b, Ly))\n",
    "\n",
    "def kernel2d_double_bilapl_tf(x, xp):\n",
    "    # first two dimensions are NxN matrix parts\n",
    "    # x is N x 1 x 2 matrix, xp is 1 x M x 2 matrix\n",
    "    Lx = L_tf[0]; Ly = L_tf[1]\n",
    "    x1 = x[:,:,0:1]\n",
    "    x2 = x[:,:,1:2]\n",
    "    x1p = xp[:,:,0:1]\n",
    "    x2p = xp[:,:,1:2]\n",
    "\n",
    "    # baseline kernel matrix which we will scale and add some terms\n",
    "    K = kernel2d_tf(x,xp)\n",
    "\n",
    "    return K * (d8_fact(x1, x1p, a, Lx) + \\\n",
    "                4.0 * d6_fact(x1, x1p, a, Lx) * d2_fact(x2, x2p, b, Ly) + \\\n",
    "                6.0 * d4_fact(x1, x1p, a, Lx) * d4_fact(x2, x2p, b, Ly) + \\\n",
    "                4.0 * d2_fact(x1, x1p, a, Lx) * d6_fact(x2, x2p, b, Ly) + \\\n",
    "                d8_fact(x2, x2p, b, Ly))\n",
    "\n",
    "def dx2_bilapl_kernel_tf(x, xp):\n",
    "    # first two dimensions are NxN matrix parts\n",
    "    # x is N x 1 x 2 matrix, xp is 1 x M x 2 matrix\n",
    "    Lx = L_tf[0]; Ly = L_tf[1]\n",
    "    x1 = x[:,:,0:1]\n",
    "    x2 = x[:,:,1:2]\n",
    "    x1p = xp[:,:,0:1]\n",
    "    x2p = xp[:,:,1:2]\n",
    "\n",
    "    # baseline kernel matrix which we will scale and add some terms\n",
    "    K = kernel2d_tf(x,xp)\n",
    "\n",
    "    # TODO : shear or general case later (just axial now)\n",
    "    return K * \\\n",
    "        (d6_fact(x1, x1p, a, Lx) +\\\n",
    "         2.0 * d4_fact(x1, x1p, a, Lx) * d2_fact(x2, x2p, b, Ly) + \\\n",
    "         d2_fact(x1, x1p, a, Lx) * d4_fact(x2, x2p, b, Ly) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the block matrix Sigma = [cov(w_int, w_int), cov(w_int, w_bndry), cov(w_int, nabla^4 w_int),\n",
    "#                                   cov(w_bndry, w_int), cov(w_bndry, w_bndry), cov(w_bndry, nabla^4 w_bndry),\n",
    "#                                   cov(nabla^4 w_int, w_int), cov(nabla^4 w_int, w_bndry), cov(nabla^4 w_int, nabla^4 w_int) ]\n",
    "# not a function of theta..\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_block = 3 * num_interior + num_bndry\n",
    "x_all = tf.concat([x_train, x_bndry], axis=0)\n",
    "num_all = num_interior + num_bndry\n",
    "\n",
    "# 11 - interior+bndry self-covariance\n",
    "x_all_L = tf.expand_dims(x_all, axis=1)\n",
    "x_all_R = tf.expand_dims(x_all, axis=0)\n",
    "K11 = tf.constant(kernel2d_tf(x_all_L, x_all_R), dtype=DTYPE)\n",
    "\n",
    "x_interior_L = tf.expand_dims(x_train, axis=1)\n",
    "x_interior_R = tf.expand_dims(x_train, axis=0)\n",
    "\n",
    "# 12 - w with Gw\n",
    "K12 = tf.constant(dx2_kernel_tf(x_all_L, x_interior_R))\n",
    "# 13 - w with nabla^4 w\n",
    "K13 = tf.constant(kernel2d_bilapl_tf(x_all_L, x_interior_R))\n",
    "# 22 - Gw with Gw\n",
    "K22 = tf.constant(doubledx2_kernel_tf(x_interior_L, x_interior_R))\n",
    "# 23 - Gw with nabla^4 w\n",
    "K23 = tf.constant(dx2_bilapl_kernel_tf(x_interior_L, x_interior_R))\n",
    "# 33 - nabla^4 w with nabla^4 w\n",
    "K33 = tf.constant(kernel2d_double_bilapl_tf(x_interior_L, x_interior_R))\n",
    "\n",
    "print(f\"{num_interior=} {num_all=}\")\n",
    "# print(f\"{K11.shape=} {K12.shape=} {K22.shape=}\")\n",
    "# print(f\"{K13.shape=} {K23.shape=} {K33.shape=}\")\n",
    "\n",
    "# assemble full covariance matrix\n",
    "_row1 = tf.constant(tf.concat([K11, K12, K13], axis=1))\n",
    "_row2 = tf.constant(tf.concat([tf.transpose(K12), K22, K23], axis=1))\n",
    "_row3 = tf.constant(tf.concat([tf.transpose(K13), tf.transpose(K23), K33], axis=1))\n",
    "Kblock_prereg = tf.concat([_row1, _row2, _row3], axis=0)\n",
    "\n",
    "print(f\"done with full matrix assembly..\")\n",
    "\n",
    "# apply robust regularization w.r.t. the max eigenvalue, namely we add epsilon * I term to matrix\n",
    "# where epsilon = alpha * lambda_max\n",
    "alpha = 1e-5\n",
    "eigvals = np.linalg.eigvalsh(Kblock_prereg)\n",
    "min_eigval = np.min(eigvals)\n",
    "max_eigval = np.max(eigvals)\n",
    "print(f\"{min_eigval=} {max_eigval=}\")\n",
    "eps = alpha * max_eigval\n",
    "\n",
    "print(\"done with eigenvalues solve\")\n",
    "\n",
    "Kblock = tf.constant(Kblock_prereg + eps * tf.eye(n_block), dtype=DTYPE)\n",
    "\n",
    "# double check eigvals again\n",
    "print(\"double check eivals again--\")\n",
    "eigvals = np.linalg.eigvalsh(Kblock)\n",
    "min_eigval = np.min(eigvals)\n",
    "max_eigval = np.max(eigvals)\n",
    "print(f\"{min_eigval=} {max_eigval=}\")\n",
    "\n",
    "# show the matrix image to see if positive definite roughly\n",
    "plt.imshow(Kblock)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define GP vectors and factor cov matrix into LU\n",
    "# -----------------------------------------------\n",
    "\n",
    "# setup lambda as trainable parameter too\n",
    "# lam = tf.Variable(pred_load_factor, trainable=True, dtype=DTYPE)\n",
    "\n",
    "K_lu = tf.linalg.lu(Kblock)\n",
    "\n",
    "w_ext = tf.zeros(shape=(num_bndry,1), dtype=DTYPE)\n",
    "# now theta includes the w int points and Gw interior points and also lam\n",
    "# theta_arr = tf.zeros(shape=(num_domain * 2 + 1,1), dtype=DTYPE)\n",
    "# theta_arr = np.random.rand(2 * num_domain + 1, 1)\n",
    "# theta_arr = tf.ones(shape=(num_domain * 2 + 1, 1), dtype=DTYPE)\n",
    "\n",
    "theta_arr = np.zeros(shape=(num_domain*2 + 1,1))\n",
    "theta_arr[-1,0] = pred_load_factor # set lam init\n",
    "\n",
    "# pick one of points nearest the middle a/2, b/2\n",
    "cent_dist = (x_train[:,0] - a/2.0)**2 + (x_train[:,1] - b/2.0)**2\n",
    "ind = tf.argmin(cent_dist)\n",
    "theta_arr[ind,0] = 1e-5 # set init value of w somewhere so not 0 norm\n",
    "\n",
    "# [5,0] = 1.0 # best to choose some middle point as the only nonzero part?\n",
    "theta = tf.Variable(theta_arr, trainable=True, dtype=DTYPE)\n",
    "\n",
    "# print(f\"{w_ext.shape=}\")\n",
    "# nabla4_w = \n",
    "# nabla4_w2 = tf.reshape(nabla4_w, shape=(num_domain,1))\n",
    "# print(f\"{nabla4_w2.shape=}\")\n",
    "\n",
    "# _temp = tf.concat([w_ext, nabla4_w2], axis=0)\n",
    "# fixed_vec = tf.constant(_temp, dtype=DTYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5000 # 5000\n",
    "learning_rate = 1e-5 # can be much higher here?\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # record the loss gradient\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        # construct full GP f vector\n",
    "        w_int = theta[:num_interior]\n",
    "        Gw = theta[num_interior:-1]\n",
    "        lam = theta[-1]\n",
    "        # lam = pred_load_factor # try constant for a minute\n",
    "        # lam = 8.0\n",
    "        nabla4_w = -lam / D * Nxx * Gw\n",
    "        w_full = tf.concat([w_int, w_ext, Gw, nabla4_w], axis=0)\n",
    "\n",
    "        # normalize w_full by w_norm to prevent trivial soln in buckling analysis\n",
    "        w_norm = tf.linalg.norm(w_int)\n",
    "        w_full_hat = w_full / w_norm\n",
    "\n",
    "        # print(f\"{w_full=}\")\n",
    "        ans = tf.linalg.lu_solve(*K_lu, rhs=w_full_hat)\n",
    "        # print(f\"{ans=}\")\n",
    "\n",
    "        loss = tf.tensordot(tf.transpose(w_full_hat), ans, axes=1)\n",
    "        loss *= 1e3\n",
    "\n",
    "        # add norm regularization term to prevent trivial soln (since buckling trivial w = 0 soln is technically a soln, but not one we care about)\n",
    "        # this didn't work very well => just focuses on this term not PDE solving\n",
    "        # loss += (w_norm - 1)**2 # * 1e4\n",
    "        # # loss *= 1e3 # *= D\n",
    "\n",
    "    loss_gradient = tape.gradient(loss, theta)\n",
    "    # print(f\"{loss_gradient=}\")\n",
    "\n",
    "    # could change to Gauss-newton algorithm here instead.. would prob be better\n",
    "\n",
    "    # gradient descent with adam optimizer\n",
    "    optimizer.apply_gradients(\n",
    "        [(loss_gradient, theta)]\n",
    "    )\n",
    "    del tape\n",
    "\n",
    "    loss_val = loss.numpy()[0,0]\n",
    "    eigval = lam.numpy()[0]\n",
    "    print(f\"{epoch=}\\t{loss_val=}\\t{eigval=}\")\n",
    "\n",
    "# print(f\"{theta=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the values at a grid of points (a mesh) so we can view predicted contour of the solution\n",
    "# using the mean interpolated from trained values\n",
    "import numpy as np\n",
    "\n",
    "n = 20\n",
    "ngrid = n * n\n",
    "_xgrid = np.linspace(0.0, a, n)\n",
    "_ygrid = np.linspace(0.0, b, n)\n",
    "X, Y = np.meshgrid(_xgrid, _ygrid)\n",
    "xgrid = np.reshape(X, newshape=(ngrid,1))\n",
    "ygrid = np.reshape(Y, newshape=(ngrid,1))\n",
    "x_grid = np.concatenate([xgrid, ygrid], axis=1)\n",
    "\n",
    "# old non-vectorized way\n",
    "# K_train = np.array([[kernel2d(x_all[i,:], x_all[j,:]) for j in range(num_all)] for i in range(num_all)])\n",
    "# K_cross = np.array([[kernel2d(x_grid[i,:], x_all[j,:]) for j in range(num_all)] for i in range(ngrid)])\n",
    "\n",
    "# vectorized way like 100x faster\n",
    "K_train = kernel2d_tf(x_all_L, x_all_R)\n",
    "x_grid_L = tf.expand_dims(tf.constant(x_grid, dtype=DTYPE), axis=1)\n",
    "K_cross = kernel2d_tf(x_grid_L, x_all_R)\n",
    "\n",
    "max_eigval = np.max(np.linalg.eigvalsh(K_train))\n",
    "W_grid = K_cross @ np.linalg.solve(K_train + 1e-5 * max_eigval * np.eye(num_all), w_full[:num_all,:])\n",
    "W = np.reshape(W_grid, newshape=(n, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# now plot the predicted solution contour\n",
    "plt.figure(figsize=(8, 6))\n",
    "colors = plt.contourf(X, Y, W, levels=20, cmap='viridis', alpha=0.75)  # Filled contours\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.colorbar(colors, label=\"w(x,y)\")  # Colorbar for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "# now plot the predicted solution surface in 3d\n",
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
    "surf = ax.plot_surface(X, Y, W, cmap=cm.coolwarm, linewidth=0, antialiased=False)\n",
    "fig.colorbar(surf, shrink=0.5, aspect=1)\n",
    "ax.set_aspect('equalxy')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "F2F",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
