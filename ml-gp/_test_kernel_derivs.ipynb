{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define vectorized versions of the kernel functions\n",
    "# like 1000x faster at assembling covariance functions\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "Lx = 0.4; Ly = 0.4\n",
    "DTYPE = tf.float32\n",
    "\n",
    "L_tf = tf.constant(np.array([Lx, Ly]), dtype=DTYPE)\n",
    "def kernel2d_tf(x, xp):\n",
    "    # x input is N x 1 x 2 array, xp is 1 x M x 2 array\n",
    "    # xbar is then an N x M x 2 shape array\n",
    "    # print(f\"{x=} {L_tf=}\")\n",
    "    xbar = (x - xp) / L_tf\n",
    "    # output is N x M matrix of kernel matrix\n",
    "    return tf.exp(-0.5 * tf.reduce_sum(tf.pow(xbar, 2.0), axis=-1))\n",
    "\n",
    "def d_fact(xbar, L):\n",
    "    return L**(-1.0) * (-xbar)\n",
    "\n",
    "def d2_fact(xbar, L):\n",
    "    return L**(-2.0) * (-1.0 + xbar**2)\n",
    "\n",
    "def d4_fact(xbar,L):\n",
    "    return L**(-4.0) * (3.0 - 6.0 * xbar**2 + xbar**4)\n",
    "\n",
    "def d6_fact(xbar,L):\n",
    "    return L**(-6.0) * (-15 + 45 * xbar**2 - 15 * xbar**4 + xbar**6)\n",
    "\n",
    "def d8_fact(xbar,L):\n",
    "    return L**(-8.0) * (105 - 420 * xbar**2 + 210 * xbar**4 - 28 * xbar**6 + xbar**8)\n",
    "\n",
    "def dx2_kernel_tf(x, xp):\n",
    "    # first two dimensions are NxN matrix parts\n",
    "    # x is N x 1 x 2 matrix, xp is 1 x M x 2 matrix\n",
    "    xbar = (x - xp) / L_tf # N x M x 2 matrix\n",
    "    x1bar = xbar[:,:,0]\n",
    "    x2bar = xbar[:,:,1]\n",
    "    Lx = L_tf[0]; Ly = L_tf[1]\n",
    "\n",
    "    # baseline kernel matrix which we will scale and add some terms\n",
    "    K = kernel2d_tf(x,xp)\n",
    "\n",
    "    # TODO : shear or general case later (just axial now)\n",
    "    return K * (d2_fact(x1bar, Lx))\n",
    "\n",
    "def dy2_kernel_tf(x, xp):\n",
    "    # first two dimensions are NxN matrix parts\n",
    "    # x is N x 1 x 2 matrix, xp is 1 x M x 2 matrix\n",
    "    xbar = (x - xp) / L_tf # N x M x 2 matrix\n",
    "    x1bar = xbar[:,:,0]\n",
    "    x2bar = xbar[:,:,1]\n",
    "    Lx = L_tf[0]; Ly = L_tf[1]\n",
    "\n",
    "    # baseline kernel matrix which we will scale and add some terms\n",
    "    K = kernel2d_tf(x,xp)\n",
    "    return K * (d2_fact(x2bar, Ly))\n",
    "\n",
    "def doubledx2_kernel_tf(x, xp):\n",
    "    # first two dimensions are NxN matrix parts\n",
    "    # x is N x 1 x 2 matrix, xp is 1 x M x 2 matrix\n",
    "    xbar = (x - xp) / L_tf # N x M x 2 matrix\n",
    "    x1bar = xbar[:,:,0]\n",
    "    x2bar = xbar[:,:,1]\n",
    "    Lx = L_tf[0]; Ly = L_tf[1]\n",
    "\n",
    "    # baseline kernel matrix which we will scale and add some terms\n",
    "    K = kernel2d_tf(x,xp)\n",
    "\n",
    "    # TODO : shear or general case later (just axial now)\n",
    "    return K * (d4_fact(x1bar, Lx))\n",
    "\n",
    "def doubledy2_kernel_tf(x, xp):\n",
    "    # first two dimensions are NxN matrix parts\n",
    "    # x is N x 1 x 2 matrix, xp is 1 x M x 2 matrix\n",
    "    xbar = (x - xp) / L_tf # N x M x 2 matrix\n",
    "    x1bar = xbar[:,:,0]\n",
    "    x2bar = xbar[:,:,1]\n",
    "    Lx = L_tf[0]; Ly = L_tf[1]\n",
    "\n",
    "    # baseline kernel matrix which we will scale and add some terms\n",
    "    K = kernel2d_tf(x,xp)\n",
    "    return K * (d4_fact(x2bar, Ly))\n",
    "\n",
    "def kernel2d_bilapl_tf(x, xp):\n",
    "    # first two dimensions are NxN matrix parts\n",
    "    # x is N x 1 x 2 matrix, xp is 1 x M x 2 matrix\n",
    "    xbar = (x - xp) / L_tf # N x M x 2 matrix\n",
    "    x1bar = xbar[:,:,0]\n",
    "    x2bar = xbar[:,:,1]\n",
    "    Lx = L_tf[0]; Ly = L_tf[1]\n",
    "\n",
    "    # baseline kernel matrix which we will scale and add some terms\n",
    "    K = kernel2d_tf(x,xp)\n",
    "\n",
    "    return K * (d4_fact(x1bar,Lx) + 2.0 * d2_fact(x1bar, Lx) * d2_fact(x2bar, Ly) + d4_fact(x2bar, Ly))\n",
    "\n",
    "def kernel2d_double_bilapl_tf(x, xp):\n",
    "    # first two dimensions are NxN matrix parts\n",
    "    # x is N x 1 x 2 matrix, xp is 1 x M x 2 matrix\n",
    "    xbar = (x - xp) / L_tf # N x M x 2 matrix\n",
    "    x1bar = xbar[:,:,0]\n",
    "    x2bar = xbar[:,:,1]\n",
    "    Lx = L_tf[0]; Ly = L_tf[1]\n",
    "\n",
    "    # baseline kernel matrix which we will scale and add some terms\n",
    "    K = kernel2d_tf(x,xp)\n",
    "\n",
    "    return K * (d8_fact(x1bar,Lx) + \\\n",
    "                4.0 * d6_fact(x1bar, Lx) * d2_fact(x2bar, Ly) +\\\n",
    "                6.0 * d4_fact(x1bar, Lx) * d4_fact(x2bar, Ly) +\\\n",
    "                4.0 * d2_fact(x1bar, Lx) * d6_fact(x2bar, Ly) +\\\n",
    "                d8_fact(x2bar, Ly))\n",
    "\n",
    "def dx2_bilapl_kernel_tf(x, xp):\n",
    "    # first two dimensions are NxN matrix parts\n",
    "    # x is N x 1 x 2 matrix, xp is 1 x M x 2 matrix\n",
    "    xbar = (x - xp) / L_tf # N x M x 2 matrix\n",
    "    x1bar = xbar[:,:,0]\n",
    "    x2bar = xbar[:,:,1]\n",
    "    Lx = L_tf[0]; Ly = L_tf[1]\n",
    "\n",
    "    # baseline kernel matrix which we will scale and add some terms\n",
    "    K = kernel2d_tf(x,xp)\n",
    "\n",
    "    # TODO : shear or general case later (just axial now)\n",
    "    return K * \\\n",
    "        (d6_fact(x1bar,Lx) + 2.0 * d4_fact(x1bar, Lx) * d2_fact(x2bar, Ly) + d2_fact(x1bar, Lx) * d4_fact(x2bar, Ly))\n",
    "\n",
    "def dy2_bilapl_kernel_tf(x, xp):\n",
    "    # first two dimensions are NxN matrix parts\n",
    "    # x is N x 1 x 2 matrix, xp is 1 x M x 2 matrix\n",
    "    xbar = (x - xp) / L_tf # N x M x 2 matrix\n",
    "    x1bar = xbar[:,:,0]\n",
    "    x2bar = xbar[:,:,1]\n",
    "    Lx = L_tf[0]; Ly = L_tf[1]\n",
    "\n",
    "    # baseline kernel matrix which we will scale and add some terms\n",
    "    K = kernel2d_tf(x,xp)\n",
    "    return K * \\\n",
    "        (d4_fact(x1bar,Lx) * d2_fact(x2bar, Ly) + 2.0 * d2_fact(x1bar, Lx) * d4_fact(x2bar, Ly) + d6_fact(x2bar, Ly))\n",
    "\n",
    "def dx2_dy2_kernel_tf(x, xp):\n",
    "    # first two dimensions are NxN matrix parts\n",
    "    # x is N x 1 x 2 matrix, xp is 1 x M x 2 matrix\n",
    "    xbar = (x - xp) / L_tf # N x M x 2 matrix\n",
    "    x1bar = xbar[:,:,0]\n",
    "    x2bar = xbar[:,:,1]\n",
    "    Lx = L_tf[0]; Ly = L_tf[1]\n",
    "\n",
    "    # baseline kernel matrix which we will scale and add some terms\n",
    "    K = kernel2d_tf(x,xp)\n",
    "    return K * d2_fact(x1bar, Lx) * d2_fact(x2bar, Ly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "dx2=<tf.Tensor: shape=(1, 1, 1), dtype=float32, numpy=array([[[-3.4815216]]], dtype=float32)> dy2=<tf.Tensor: shape=(1, 1, 1), dtype=float32, numpy=array([[[0.4954788]]], dtype=float32)>\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "dx2_val=-3.4815216 analytic_dx2=-3.4815216 dx2_rel_err=0.0\n",
      "dy2_val=0.4954788 analytic_dy2=0.495479 dy2_rel_err=3.608912e-07\n",
      "tf_bilapl_x=2.608574 analytic_bilapl_x=2.6085684 bilapl_rel_err=2.1021553e-06\n"
     ]
    }
   ],
   "source": [
    "# verify the kernel function derivatives with tensorflow gradient taping..\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "xy = np.random.rand(1,1,2).astype(np.float32)\n",
    "x = tf.constant(xy[:,:,0:1]); y = tf.constant(xy[:,:,1:2])\n",
    "# temp = tf.concat([x, y], axis=0)\n",
    "xyp = tf.constant(np.random.rand(1,1,2).astype(np.float32))\n",
    "xp = tf.constant(xyp[:, :, 0:1]); yp = tf.constant(xyp[:, :, 1:2])\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape4:\n",
    "    tape4.watch(x)\n",
    "    tape4.watch(y)\n",
    "    with tf.GradientTape(persistent=True) as tape3:\n",
    "        tape3.watch(x)\n",
    "        tape3.watch(y)\n",
    "        with tf.GradientTape(persistent=True) as tape2:\n",
    "            tape2.watch(x)\n",
    "            tape2.watch(y)\n",
    "            with tf.GradientTape(persistent=True) as tape1:\n",
    "                tape1.watch(x)\n",
    "                tape1.watch(y)\n",
    "                K = kernel2d_tf(tf.concat([x, y], axis=2), xyp)\n",
    "                dKdx = tape1.gradient(K, x)\n",
    "                dKdy = tape1.gradient(K, y)\n",
    "                # print(f\"{K=} {dKdx=} {dKdy=}\")\n",
    "            dx2 = tape2.gradient(dKdx, x)\n",
    "            dy2 = tape2.gradient(dKdy, y)\n",
    "            print(f\"{dx2=} {dy2=}\")\n",
    "        dx3 = tape3.gradient(dx2, x)\n",
    "        dxxy = tape3.gradient(dx2, y)\n",
    "        dy3 = tape3.gradient(dy2, y)\n",
    "    dx4 = tape4.gradient(dx3, x)\n",
    "    dx2y2 = tape4.gradient(dxxy, y)\n",
    "    dy4 = tape4.gradient(dy3, y)\n",
    "\n",
    "del tape1, tape2, tape3, tape4\n",
    "\n",
    "# test dx2 k_x derivs\n",
    "analytic_dx2 = dx2_kernel_tf(xy, xyp).numpy()[0,0]\n",
    "dx2_val = dx2.numpy()[0,0,0]\n",
    "dx2_rel_err = abs((analytic_dx2 - dx2_val) / dx2_val)\n",
    "print(f\"{dx2_val=} {analytic_dx2=} {dx2_rel_err=}\")\n",
    "\n",
    "# test dy2 k_x derivs\n",
    "analytic_dy2 = dy2_kernel_tf(xy, xyp).numpy()[0,0]\n",
    "dy2_val = dy2.numpy()[0,0,0]\n",
    "dy2_rel_err = abs((analytic_dy2 - dy2_val) / dy2_val)\n",
    "print(f\"{dy2_val=} {analytic_dy2=} {dy2_rel_err=}\")\n",
    "\n",
    "# the nabla^4_x k_x for 2d kernel seems to be correct!\n",
    "tf_bilapl_x = (dx4 + 2.0 * dx2y2 + dy4).numpy()[0,0,0]\n",
    "analytic_bilapl_x = kernel2d_bilapl_tf(xy, xyp).numpy()[0,0]\n",
    "bilapl_rel_err = np.abs((tf_bilapl_x - analytic_bilapl_x) / tf_bilapl_x)\n",
    "print(f\"{tf_bilapl_x=} {analytic_bilapl_x=} {bilapl_rel_err=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "doubledx2_val=64.639046 analytic_doubledx2=64.639046 doubledx2_rel_err=0.0\n",
      "doubledy2_val=-55.897263 analytic_doubledy2=-55.897266 doubledy2_rel_err=6.8244795e-08\n",
      "analytic_dx2_dyp2=-3.066605 dx2_dyp2_val=-3.0666041 dx2_dyp2_rel_err=3.109871e-07\n"
     ]
    }
   ],
   "source": [
    "# check dx2, dy2 cross derivs here (4th order)\n",
    "with tf.GradientTape(persistent=True) as tape4:\n",
    "    tape4.watch(xp)\n",
    "    tape4.watch(yp)\n",
    "    with tf.GradientTape(persistent=True) as tape3:\n",
    "        tape3.watch(xp)\n",
    "        tape3.watch(yp)\n",
    "        with tf.GradientTape(persistent=True) as tape2:\n",
    "            tape2.watch(x)\n",
    "            tape2.watch(y)\n",
    "            with tf.GradientTape(persistent=True) as tape1:\n",
    "                tape1.watch(x)\n",
    "                tape1.watch(y)\n",
    "                K = kernel2d_tf(tf.concat([x, y], axis=2), tf.concat([xp, yp], axis=2))\n",
    "                dKdx = tape1.gradient(K, x)\n",
    "                dKdy = tape1.gradient(K, y)\n",
    "                # print(f\"{K=} {dKdx=} {dKdy=}\")\n",
    "            dx2 = tape2.gradient(dKdx, x)\n",
    "            dy2 = tape2.gradient(dKdy, y)\n",
    "            # print(f\"{dx2=} {dy2=}\")\n",
    "        dx2_dxp = tape3.gradient(dx2, xp)\n",
    "        dy2_dyp = tape3.gradient(dy2, yp)\n",
    "        dx2_dyp = tape3.gradient(dx2, yp)\n",
    "    dx2_dxp2 = tape4.gradient(dx2_dxp, xp)\n",
    "    dy2_dyp2 = tape4.gradient(dy2_dyp, yp)\n",
    "    dx2_dyp2 = tape4.gradient(dx2_dyp, yp)\n",
    "\n",
    "# test double dx2 k_x derivs\n",
    "analytic_doubledx2 = doubledx2_kernel_tf(xy, xyp).numpy()[0,0]\n",
    "doubledx2_val = dx2_dxp2.numpy()[0,0,0]\n",
    "doubledx2_rel_err = abs((analytic_doubledx2 - doubledx2_val) / doubledx2_val)\n",
    "print(f\"{doubledx2_val=} {analytic_doubledx2=} {doubledx2_rel_err=}\")\n",
    "\n",
    "# test double dy2 k_x derivs\n",
    "analytic_doubledy2 = doubledy2_kernel_tf(xy, xyp).numpy()[0,0]\n",
    "doubledy2_val = dy2_dyp2.numpy()[0,0,0]\n",
    "doubledy2_rel_err = abs((analytic_doubledy2 - doubledy2_val) / doubledy2_val)\n",
    "print(f\"{doubledy2_val=} {analytic_doubledy2=} {doubledy2_rel_err=}\")\n",
    "\n",
    "# test mixed dx2 dyp2 derivs\n",
    "analytic_dx2_dyp2 = dx2_dy2_kernel_tf(xy, xyp).numpy()[0,0]\n",
    "dx2_dyp2_val = dx2_dyp2.numpy()[0,0,0]\n",
    "dx2_dyp2_rel_err = abs((analytic_dx2_dyp2 - dx2_dyp2_val) / dx2_dyp2_val)\n",
    "print(f\"{analytic_dx2_dyp2=} {dx2_dyp2_val=} {dx2_dyp2_rel_err=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "bilapl_dxp2_val=-1540.2228 analytic_bilapl_dxp2=-1540.2225 dx2_bilapl_rel_err=1.5850995e-07\n",
      "bilapl_dyp2_val=3262.2231 analytic_bilapl_dyp2=3262.2227 dy2_bilapl_rel_err=1.4967745e-07\n"
     ]
    }
   ],
   "source": [
    "# check 6th order derivs for cross-covariance btw dx2, dy2 and bilapl xyp\n",
    "with tf.GradientTape(persistent=True) as tape6:\n",
    "    tape6.watch(xp)\n",
    "    tape6.watch(yp)\n",
    "    with tf.GradientTape(persistent=True) as tape5:\n",
    "        tape5.watch(xp)\n",
    "        tape5.watch(yp)\n",
    "        with tf.GradientTape(persistent=True) as tape4:\n",
    "            tape4.watch(x)\n",
    "            tape4.watch(y)\n",
    "            with tf.GradientTape(persistent=True) as tape3:\n",
    "                tape3.watch(x)\n",
    "                tape3.watch(y)\n",
    "                with tf.GradientTape(persistent=True) as tape2:\n",
    "                    tape2.watch(x)\n",
    "                    tape2.watch(y)\n",
    "                    with tf.GradientTape(persistent=True) as tape1:\n",
    "                        tape1.watch(x)\n",
    "                        tape1.watch(y)\n",
    "                        K = kernel2d_tf(tf.concat([x, y], axis=2), tf.concat([xp, yp], axis=2))\n",
    "                        dKdx = tape1.gradient(K, x)\n",
    "                        dKdy = tape1.gradient(K, y)\n",
    "                        # print(f\"{K=} {dKdx=} {dKdy=}\")\n",
    "                    dx2 = tape2.gradient(dKdx, x)\n",
    "                    dy2 = tape2.gradient(dKdy, y)\n",
    "                    # print(f\"{dx2=} {dy2=}\")\n",
    "                dx3 = tape3.gradient(dx2, x)\n",
    "                dxxy = tape3.gradient(dx2, y)\n",
    "                dy3 = tape3.gradient(dy2, y)\n",
    "            dx4 = tape4.gradient(dx3, x)\n",
    "            dx2y2 = tape4.gradient(dxxy, y)\n",
    "            dy4 = tape4.gradient(dy3, y)\n",
    "            bilapl = dx4 + 2.0 * dx2y2 + dy4\n",
    "        bilapl_dxp = tape5.gradient(bilapl, xp)\n",
    "        bilapl_dyp = tape5.gradient(bilapl, yp)\n",
    "    bilapl_dxp2 = tape5.gradient(bilapl_dxp, xp)\n",
    "    bilapl_dyp2 = tape5.gradient(bilapl_dyp, yp)\n",
    "        \n",
    "# test bilapl with dxp2 k_x derivs\n",
    "analytic_bilapl_dxp2 = dx2_bilapl_kernel_tf(xy, xyp).numpy()[0,0]\n",
    "bilapl_dxp2_val = bilapl_dxp2.numpy()[0,0,0]\n",
    "dx2_bilapl_rel_err = abs((analytic_bilapl_dxp2 - bilapl_dxp2_val) / bilapl_dxp2_val)\n",
    "print(f\"{bilapl_dxp2_val=} {analytic_bilapl_dxp2=} {dx2_bilapl_rel_err=}\")\n",
    "\n",
    "# test bilapl with dxp2 k_x derivs\n",
    "analytic_bilapl_dyp2 = dy2_bilapl_kernel_tf(xy, xyp).numpy()[0,0]\n",
    "bilapl_dyp2_val = bilapl_dyp2.numpy()[0,0,0]\n",
    "dy2_bilapl_rel_err = abs((analytic_bilapl_dyp2 - bilapl_dyp2_val) / bilapl_dyp2_val)\n",
    "print(f\"{bilapl_dyp2_val=} {analytic_bilapl_dyp2=} {dy2_bilapl_rel_err=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf_bilaplx_bilaplxp=<tf.Tensor: shape=(1, 1, 1), dtype=float32, numpy=array([[[-141959.31]]], dtype=float32)>\n",
      "analytic_bilaplx_bilaplxp=<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[-141959.27]], dtype=float32)>\n",
      "double_bilapl_rel_err=<tf.Tensor: shape=(1, 1, 1), dtype=float32, numpy=array([[[3.3020035e-07]]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "# verify 8th order derivatives\n",
    "\n",
    "# now check the bilaplacian with 8th order derivatives..\n",
    "# namely nabla_x^4 \\odot nabla_{x'}^4 \\odot K(x,x') with x,x' in R^2\n",
    "with tf.GradientTape(persistent=True) as tape8:\n",
    "    tape8.watch(xp)\n",
    "    tape8.watch(yp)\n",
    "    with tf.GradientTape(persistent=True) as tape7:\n",
    "        tape7.watch(xp)\n",
    "        tape7.watch(yp)\n",
    "        with tf.GradientTape(persistent=True) as tape6:\n",
    "            tape6.watch(xp)\n",
    "            tape6.watch(yp)\n",
    "            with tf.GradientTape(persistent=True) as tape5:\n",
    "                tape5.watch(xp)\n",
    "                tape5.watch(yp)\n",
    "                with tf.GradientTape(persistent=True) as tape4:\n",
    "                    tape4.watch(x)\n",
    "                    tape4.watch(y)\n",
    "                    with tf.GradientTape(persistent=True) as tape3:\n",
    "                        tape3.watch(x)\n",
    "                        tape3.watch(y)\n",
    "                        with tf.GradientTape(persistent=True) as tape2:\n",
    "                            tape2.watch(x)\n",
    "                            tape2.watch(y)\n",
    "                            with tf.GradientTape(persistent=True) as tape1:\n",
    "                                tape1.watch(x)\n",
    "                                tape1.watch(y)\n",
    "                                K = kernel2d_tf(tf.concat([x, y], axis=2), tf.concat([xp, yp], axis=2))\n",
    "                                dKdx = tape1.gradient(K, x)\n",
    "                                dKdy = tape1.gradient(K, y)\n",
    "                                # print(f\"{K=} {dKdx=} {dKdy=}\")\n",
    "                            dx2 = tape2.gradient(dKdx, x)\n",
    "                            dy2 = tape2.gradient(dKdy, y)\n",
    "                            # print(f\"{dx2=} {dy2=}\")\n",
    "                        dx3 = tape3.gradient(dx2, x)\n",
    "                        dxxy = tape3.gradient(dx2, y)\n",
    "                        dy3 = tape3.gradient(dy2, y)\n",
    "                    dx4 = tape4.gradient(dx3, x)\n",
    "                    dx2y2 = tape4.gradient(dxxy, y)\n",
    "                    dy4 = tape4.gradient(dy3, y)\n",
    "                dxp_dx4 = tape5.gradient(dx4, xp)\n",
    "                dxp_dx2y2 = tape5.gradient(dx2y2, xp)\n",
    "                dxp_dy4 = tape5.gradient(dy4, xp)\n",
    "                dyp_dx4 = tape5.gradient(dx4, yp)\n",
    "                dyp_dx2y2 = tape5.gradient(dx2y2, yp)\n",
    "                dyp_dy4 = tape5.gradient(dy4, yp)\n",
    "            dxp2_dx4 = tape6.gradient(dxp_dx4,xp)\n",
    "            dyp2_dx4 = tape6.gradient(dyp_dx4, yp)\n",
    "            dxp2_dx2y2 = tape6.gradient(dxp_dx2y2,xp)\n",
    "            dyp2_dx2y2 = tape6.gradient(dyp_dx2y2, yp)\n",
    "            dxp2_dy4 = tape6.gradient(dxp_dy4,xp)\n",
    "            dyp2_dy4 = tape6.gradient(dyp_dy4, yp)\n",
    "        dxp3_dx4 = tape7.gradient(dxp2_dx4, xp)\n",
    "        dxpxpyp_dx4 = tape7.gradient(dxp2_dx4, yp)\n",
    "        dyp3_dx4 = tape7.gradient(dyp2_dx4, yp)\n",
    "        dxp3_dx2y2 = tape7.gradient(dxp2_dx2y2, xp)\n",
    "        dxpxpyp_dx2y2 = tape7.gradient(dxp2_dx2y2, yp)\n",
    "        dyp3_dx2y2 = tape7.gradient(dyp2_dx2y2, yp)\n",
    "        dxp3_dy4 = tape7.gradient(dxp2_dy4, xp)\n",
    "        dxpxpyp_dy4 = tape7.gradient(dxp2_dy4, yp)\n",
    "        dyp3_dy4 = tape7.gradient(dyp2_dy4, yp)\n",
    "    dxp4_dx4 = tape8.gradient(dxp3_dx4, xp)\n",
    "    dxp2yp2_dx4 = tape8.gradient(dxpxpyp_dx4, yp)\n",
    "    dyp4_dx4 = tape8.gradient(dyp3_dx4, yp)\n",
    "    dxp4_dx2y2 = tape8.gradient(dxp3_dx2y2, xp)\n",
    "    dxp2yp2_dx2y2 = tape8.gradient(dxpxpyp_dx2y2, yp)\n",
    "    dyp4_dx2y2 = tape8.gradient(dyp3_dx2y2, yp)\n",
    "    dxp4_dy4 = tape8.gradient(dxp3_dy4, xp)\n",
    "    dxp2yp2_dy4 = tape8.gradient(dxpxpyp_dy4, yp)\n",
    "    dyp4_dy4 = tape8.gradient(dyp3_dy4, yp)\n",
    "\n",
    "tf_bilaplx_bilaplxp = dxp4_dx4 + 2.0 * dxp4_dx2y2 + dxp4_dy4 + \\\n",
    "                      2.0 * (dxp2yp2_dx4 + 2.0 * dxp2yp2_dx2y2 + dxp2yp2_dy4) + \\\n",
    "                      dyp4_dx4 + 2.0 * dyp4_dx2y2 + dyp4_dy4\n",
    "analytic_bilaplx_bilaplxp = kernel2d_double_bilapl_tf(xy, xyp)\n",
    "double_bilapl_rel_err = abs((tf_bilaplx_bilaplxp-analytic_bilaplx_bilaplxp)/analytic_bilaplx_bilaplxp)\n",
    "\n",
    "\n",
    "print(f\"{tf_bilaplx_bilaplxp=}\")\n",
    "print(f\"{analytic_bilaplx_bilaplxp=}\")\n",
    "print(f\"{double_bilapl_rel_err=}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "F2F",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
